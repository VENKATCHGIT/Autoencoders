{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Encoders in Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder is an  artificial neural network that learns how to efficiently compress and encode data then learns how to reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible.\n",
    "\n",
    "#### When ? - If you have additional unlabeled data (beyond the labelled training data). e.g. If you are recognizing employee faces do could use unlabeled face images (of any face).\n",
    "\n",
    "#### By making use of (plentiful) unlabelled data â€“ to generate a reduced dimension encoding. As the encoding is reduced dimension, the amount of labelled training data to train a classifier is reduced.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train,_), (x_test,_) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x63945b3d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAO0ElEQVR4nO3de4xU53nH8d/DsgaHhIbrdg00hIDlGCNDu4bWthJcN5FjpcaJmzioibBqlVSFNLFQU1+k2FGlilaNXTvNpbgmJk6CG/kS08SKgxARjZxaLARzKeYSgvEaArGxDBgDu8vTP/YQbfCed5Y5M3PGPN+PNJqZ88yZ8zDw48zMO+e85u4CcP4bUnYDABqDsANBEHYgCMIOBEHYgSCGNnJjF9gwH64RjdwkEMoJvaFTftIGqhUKu5ldJ+l+SS2S/tPdl6YeP1wjNMeuLbJJAAnP+ZrcWtVv482sRdLXJH1E0qWS5pvZpdU+H4D6KvKZfbak3e6+x91PSXpU0rzatAWg1oqEfYKkl/rd78qW/Q4zW2hmnWbW2a2TBTYHoIgiYR/oS4C3/PbW3Ze5e4e7d7RqWIHNASiiSNi7JE3qd3+ipP3F2gFQL0XCvl7SNDN7r5ldIOlTklbVpi0AtVb10Ju795jZYknPqG/obbm7b6tZZwBqqtA4u7s/LenpGvUCoI74uSwQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBFJrFFWgZMzpZt98bmVvbd9NFyXVPjPVkfeqXn0/WTx8/nqxHUyjsZrZX0lFJvZJ63L2jFk0BqL1a7NmvcfdXavA8AOqIz+xAEEXD7pJ+YmYbzGzhQA8ws4Vm1mlmnd06WXBzAKpV9G38Ve6+38zGS1ptZi+4+7r+D3D3ZZKWSdJIG53+xgVA3RTas7v7/uz6kKQnJc2uRVMAaq/qsJvZCDN715nbkj4saWutGgNQW0XexrdJetLMzjzP99z9xzXpCg0z5LJLkvVdd1yYrP/VjGeT9SVjnjnnngbr/W1/k6xPu2VD3bb9dlR12N19j6TLa9gLgDpi6A0IgrADQRB2IAjCDgRB2IEgOMT1PGBXzMit7b6tJbnuT6/+92R9XMuwZH1Ihf3Fj46Pyq3tOTk+ue6iUTuS9Uc+8GCy/o9XLMit+fotyXXPR+zZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIxtmbQMu4ccn6zvsnJOv/feXXc2tTWlsrbD09jl7Jt45MStZ/cNPVubXTw9K9Lfphepy9Y1hvsv5mW/7hucOTa56f2LMDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCMszeBlz89LVnf9sH7KzxDpbH06n2n0jj6jVcm6707dubWbNb0qnpCddizA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQjLM3gQk37K3bcz927PeT9Xt3Xpust33Rk/XeHbvOuaczXpsxsup1ce4q7tnNbLmZHTKzrf2WjTaz1Wa2K7vOnwkAQFMYzNv4hyVdd9ay2yWtcfdpktZk9wE0sYphd/d1kg6ftXiepBXZ7RWSbqxxXwBqrNov6Nrc/YAkZde5k3aZ2UIz6zSzzm6drHJzAIqq+7fx7r7M3TvcvaO14MkNAVSv2rAfNLN2ScquD9WuJQD1UG3YV0k6Mx/uAklP1aYdAPVScZzdzFZKmitprJl1Sbpb0lJJ3zezWyXtk/SJejZ53vvr9MebSxd9LlmftDr//Okjtv06ue7YF/OPN5ek9JnZizneZnV8dpytYtjdfX5OKf1rDABNhZ/LAkEQdiAIwg4EQdiBIAg7EASHuDaB3t2/Stan3paup/RUvWb9dV9xtOwWQmHPDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBMM4e3L4vpadc7nlH+lTSqnSUamL1j0/7eYWV0xZ3zU3WL/zxxtxahT/VeYk9OxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwTj720DLyPTUxidmT8uttd5xMLnu5ku+WlVPv31+a0nWu736k1GvffMdyXrXwj9I1r1ne9XbPh+xZweCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIBhnbwAblp6S+dQHZyTrt339kWT9mgvX5NYO9p5Mrrv2zVHJ+pd2zkvWV05/OFm/aGj6z54yfEh3sr7nk+9O1qfsGJ5bO33iRFU9vZ1V3LOb2XIzO2RmW/stu8fMXjazTdnl+vq2CaCowbyNf1jSdQMsv8/dZ2aXp2vbFoBaqxh2d18n6XADegFQR0W+oFtsZpuzt/m5H/zMbKGZdZpZZ7fSnx8B1E+1Yf+GpPdJminpgKSv5D3Q3Ze5e4e7d7Sq+i9rABRTVdjd/aC797r7aUkPSppd27YA1FpVYTez9n53PyZpa95jATSHiuPsZrZS0lxJY82sS9Ldkuaa2Uz1nX57r6TP1rHHpjdkeP54riS9evOsZP1//umBQtufvvJzubWJa9PHkw/70fpkfUz7sWR95TN/lKwvGVP9fmDOsPQ4++Zb0q/bn7z0d7m1tm8/n1z39PHjyfrbUcWwu/v8ARY/VIdeANQRP5cFgiDsQBCEHQiCsANBEHYgCHNv3OS1I220z7FrG7a9WkodprrjvsuT674w72uFtj1vx43J+pD5+UNUvQcPJdcdOmlisn75qn3J+pfH/yJZf/10/qGkcx5fkly3/ZJ072tm/FeynnLz7o8m6688MDlZH/5qeliwkpaf5k8nXcRzvkZH/PCAE2mzZweCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIDiVdMaGpl+KHf+WP5b+wg3pcfSunvTpuG74jy8m65OX/zJZ70mMpXf/WfoQ1Mv+OT1Ofvf4Dcn6t468J1l/5K4/z61NfeJ/k+u2jB2TrM/9UP6hvZL0xs2v59aenPVgct2JDxQ7q9IP30j3vuziKYWevxrs2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCI5nz3TdcWWyvnHx/bm1/RXG0W9a+vfJevsPfpWsH75mcrLun34lt/bYZQ8n1x3Xkh5Pnv5oeiz74mX525ak3h27k/WyHPrb9N9321+8WGwDS9LTSfsvthV7/hwczw6AsANREHYgCMIOBEHYgSAIOxAEYQeCYJw9c9eeTcl6avrgw73pcfZvvjYnWZ9wwWvJ+oKRBcd8E6Z/L39aY0maekd6Smfv6allOyio0Di7mU0ys7Vmtt3MtpnZ57Plo81stZntyq5H1bpxALUzmLfxPZKWuPv7Jf2xpEVmdqmk2yWtcfdpktZk9wE0qYphd/cD7r4xu31U0nZJEyTNk7Qie9gKSek5igCU6py+oDOzyZJmSXpOUpu7H5D6/kOQND5nnYVm1mlmnd1Kf7YFUD+DDruZvVPS45K+4O5HBrueuy9z9w5372hVsZP4AajeoMJuZq3qC/p33f2JbPFBM2vP6u2S0lNuAihVxVNJm5lJekjSdne/t19plaQFkpZm10/VpcMGWXfskmR9zrAtubXRFQ4TvXNselivko++8PFkfd/P86ddnvJY/umUJWnqtvSpohlaO38M5rzxV0n6jKQtZnbmX+2d6gv5983sVkn7JH2iPi0CqIWKYXf3n0kacJBeUnP+QgbAW/BzWSAIwg4EQdiBIAg7EARhB4JgyubMs9dclKzP+cs/za29fvmp5LpDf9OarF/8zZfT6/86/XulySdeyq2dTq6JSNizA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQjLNnel89nKy3PfBsfq3gtjliHI3Anh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCqBh2M5tkZmvNbLuZbTOzz2fL7zGzl81sU3a5vv7tAqjWYE5e0SNpibtvNLN3SdpgZquz2n3u/q/1aw9ArQxmfvYDkg5kt4+a2XZJE+rdGIDaOqfP7GY2WdIsSc9lixab2WYzW25mo3LWWWhmnWbW2a2ThZoFUL1Bh93M3inpcUlfcPcjkr4h6X2SZqpvz/+VgdZz92Xu3uHuHa0aVoOWAVRjUGE3s1b1Bf277v6EJLn7QXfvdffTkh6UNLt+bQIoajDfxpukhyRtd/d7+y1v7/ewj0naWvv2ANTKYL6Nv0rSZyRtMbNN2bI7Jc03s5mSXNJeSZ+tS4cAamIw38b/TJINUHq69u0AqBd+QQcEQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQjC3L1xGzP7jaQX+y0aK+mVhjVwbpq1t2btS6K3atWyt/e4+7iBCg0N+1s2btbp7h2lNZDQrL01a18SvVWrUb3xNh4IgrADQZQd9mUlbz+lWXtr1r4keqtWQ3or9TM7gMYpe88OoEEIOxBEKWE3s+vMbIeZ7Taz28voIY+Z7TWzLdk01J0l97LczA6Z2dZ+y0ab2Woz25VdDzjHXkm9NcU03olpxkt97cqe/rzhn9nNrEXSTkkfktQlab2k+e7+fw1tJIeZ7ZXU4e6l/wDDzD4g6Zikb7v7Zdmyf5F02N2XZv9RjnL3f2iS3u6RdKzsabyz2Yra+08zLulGSbeoxNcu0dcn1YDXrYw9+2xJu919j7ufkvSopHkl9NH03H2dpMNnLZ4naUV2e4X6/rE0XE5vTcHdD7j7xuz2UUlnphkv9bVL9NUQZYR9gqSX+t3vUnPN9+6SfmJmG8xsYdnNDKDN3Q9Iff94JI0vuZ+zVZzGu5HOmma8aV67aqY/L6qMsA80lVQzjf9d5e5/KOkjkhZlb1cxOIOaxrtRBphmvClUO/15UWWEvUvSpH73J0raX0IfA3L3/dn1IUlPqvmmoj54Zgbd7PpQyf38VjNN4z3QNONqgteuzOnPywj7eknTzOy9ZnaBpE9JWlVCH29hZiOyL05kZiMkfVjNNxX1KkkLstsLJD1VYi+/o1mm8c6bZlwlv3alT3/u7g2/SLpefd/I/1LSXWX0kNPXFEnPZ5dtZfcmaaX63tZ1q+8d0a2SxkhaI2lXdj26iXp7RNIWSZvVF6z2knq7Wn0fDTdL2pRdri/7tUv01ZDXjZ/LAkHwCzogCMIOBEHYgSAIOxAEYQeCIOxAEIQdCOL/Ab+hZHhXLzvmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[randint(0, x_train.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-sizing the shape of data\n",
    "#### Each training example is of size 28 x 28 (2-d matrix). But in the machine learning world \n",
    "#### we assume training examples as rows and features as columns. Hence, reshaping 28 x 28 image to row of 784 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing the data\n",
    "#### Dividing with 255 since the pixel ranges between 0-255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we are following the Functional Mode for building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(784,))\n",
    "encoding_layer = Dense(32, activation='relu')(input_layer)\n",
    "decoding_layer = Dense(784, activation='sigmoid')(encoding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(input_layer, decoding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 784)               25872     \n",
      "=================================================================\n",
      "Total params: 50,992\n",
      "Trainable params: 50,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(loss='binary_crossentropy', optimizer='adagrad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding layer\n",
    "#### Forming the encoding layer from total model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_layer, encoding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding layer\n",
    "#### Forming the decoding layer from total model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = Input(shape=(32,))\n",
    "decoded_output = autoencoder.layers[-1](encoded_input)\n",
    "decoder = Model(encoded_input, decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 784])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.1692 - val_loss: 0.1421\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.1339 - val_loss: 0.1273\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 2s 46us/step - loss: 0.1230 - val_loss: 0.1196\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.1168 - val_loss: 0.1147\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.1126 - val_loss: 0.1113\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.1097 - val_loss: 0.1089\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.1074 - val_loss: 0.1069\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.1057 - val_loss: 0.1054\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.1042 - val_loss: 0.1042\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.1030 - val_loss: 0.1031\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.1020 - val_loss: 0.1022\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.1012 - val_loss: 0.1014\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.1005 - val_loss: 0.1008\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.0998 - val_loss: 0.1002\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0993 - val_loss: 0.0997\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.0988 - val_loss: 0.0993\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0984 - val_loss: 0.0989\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.0980 - val_loss: 0.0986\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.0977 - val_loss: 0.0983\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.0975 - val_loss: 0.0981\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0972 - val_loss: 0.0979\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.0970 - val_loss: 0.0977\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.0968 - val_loss: 0.0975\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.0967 - val_loss: 0.0974\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.0965 - val_loss: 0.0972\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.0964 - val_loss: 0.0971\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.0963 - val_loss: 0.0970\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0962 - val_loss: 0.0969\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0961 - val_loss: 0.0968\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.0960 - val_loss: 0.0967\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.0959 - val_loss: 0.0967\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.0958 - val_loss: 0.0966\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.0957 - val_loss: 0.0965\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0957 - val_loss: 0.0965\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.0956 - val_loss: 0.0964\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0956 - val_loss: 0.0963\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.0955 - val_loss: 0.0963\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.0955 - val_loss: 0.0962\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.0954 - val_loss: 0.0962\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.0954 - val_loss: 0.0962\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0953 - val_loss: 0.0961\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0953 - val_loss: 0.0961\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0952 - val_loss: 0.0961\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0952 - val_loss: 0.0960\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0952 - val_loss: 0.0960\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0951 - val_loss: 0.0960\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0951 - val_loss: 0.0960\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0951 - val_loss: 0.0959\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.0951 - val_loss: 0.0959\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.0950 - val_loss: 0.0959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x653991b90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x=x_train, y=x_train, batch_size=32, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_encoded = encoder.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = encoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding on test encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_decoded = decoder.predict(x_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reshape(x_test.shape[0], 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_decoded = x_test_decoded.reshape(x_test_decoded.shape[0], 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_decoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking the random image from test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3393"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = randint(0, x_test.shape[0])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x653971350>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test_decoded[img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.layers[1].trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Auto-encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Anamoly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above encoding and decoding is on MNIST data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if the img.png is not one of the MNIST dataset that the model was trained on, the error will be very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/i313121'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/i313121/Desktop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5318.406]\n"
     ]
    }
   ],
   "source": [
    "img_anamoly = image.load_img(\"./venkatasatyanarayana.png\", target_size=(28, 28), color_mode = \"grayscale\")\n",
    "input_img = image.img_to_array(img_anamoly)\n",
    "inputs = input_img.reshape(1,784)\n",
    "target_data = autoencoder.predict(inputs)\n",
    "dist = np.linalg.norm(inputs - target_data, axis=-1)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### During the training we have seen that validation error for last epoch for MNIST data is 0.0943"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However for above normal picture the error is 5317.6865"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Noise reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-encoders help in reducing the noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since above model is trained on MNIST data\n",
    "### Let's introduce some NOISE to it and clear the denoise it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  \n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  \n",
    "\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x632f9b410>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test_noisy[1].reshape(28, 28)) ##Noisy Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_noisy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-87-87b999f33f71>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-87-87b999f33f71>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    cnnencoder = Model(input_img, cnndecoded)\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape=(28, 28, 1))\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "cnnencoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (7, 7, 32)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(cnnencoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "cnndecoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "cnnencoder = Model(input_img, cnndecoded)\n",
    "cnnencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 110s 2ms/step - loss: 0.1714 - val_loss: 0.1250\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.1141 - val_loss: 0.1088\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 127s 2ms/step - loss: 0.1079 - val_loss: 0.1042\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 129s 2ms/step - loss: 0.1049 - val_loss: 0.1017\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 130s 2ms/step - loss: 0.1031 - val_loss: 0.1012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x634f01b90>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnnencoder.fit(x_train_noisy, x_train,\n",
    "                epochs=5,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_noisy, x_test),\n",
    "                callbacks=[TensorBoard(log_dir='/tmp/tb', histogram_freq=0, write_graph=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_encoder = Model(input_img, cnnencoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_encoded_input = Input(shape=(7,7,32))\n",
    "cnn_decoded_output = cnnencoder.layers[-1](cnn_encoded_input)\n",
    "cnn_decoder = Model(cnn_encoded_input, cnn_decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = cnn_encoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7, 7, 32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_decoded = cnn_decoder.predict(x_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7, 7, 1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_noisy = x_test_noisy[1]\n",
    "input_nimg = image.img_to_array(img_noisy)\n",
    "target_data = cnn_decoder.predict(input_nimg)\n",
    "dist = np.linalg.norm(input_img - target_data, axis=-1)\n",
    "print(dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
